{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1177901",
   "metadata": {},
   "source": [
    "Data Source:\n",
    "[1] Almeida, T.A., G�mez Hidalgo, J.M., Yamakami, A. Contributions to the study of SMS Spam Filtering: New Collection and Results. Proceedings of the 2011 ACM Symposium on Document Engineering (ACM DOCENG'11), Mountain View, CA, USA, 2011. (Under review)\n",
    "http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/ \n",
    "\n",
    "Data is taken from primarily Singaporean colledge students, so the model may not apply as well to other places or groups. However the process here would be applicable to any other spam classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b4b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dab635",
   "metadata": {},
   "source": [
    "The first step here will be to import the data. We also change the label to be a boolean integer, where 0 is the negative (no spam) and 1 is the positive (spam) case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84de609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "#Import data\n",
    "df = pd.read_csv('Data\\\\SMSSPamCollection',names=['label','text'],sep='\\t',encoding='utf-8')\n",
    "\n",
    "#Format Data\n",
    "spamdict = {'ham' : 0, 'spam' : 1}\n",
    "df['label'] = df['label'].apply(lambda x: spamdict[x])\n",
    "print(df.head()) #Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4acb33",
   "metadata": {},
   "source": [
    "This dataset does not contain missing values. We should check for any unusual characters and clean them from the set, so that the model is not training on them. I also had some issues with UTF-8 encoding specifically when running from the dockerfile, so we want to ensure that all of the characters are compatible.\n",
    "\n",
    " We can keep punctuation in the data however. Even though they often don't have semantic meaning, punctuation can be an indicator of spam. Casual (human) texts often don't include punctuation, and spam texts will often make liberal use of it. For instance, a spam text that says \"URGENT!!!\" to grab the reader's attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bae78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Unusual Character Check\n",
    "df['text'] = df['text'].str.replace('£', '$') #Same meaning semantically. May cause compability issues.\n",
    "\n",
    "pattern_unusual = r'[^a-zA-Z0-9\\sa!@#$%^&*()_+\\-=\\[\\]{};\\':\"\\\\|,.<>\\/?]'\n",
    "\n",
    "df['text'] = df['text'].str.replace('\\u2018', '\\'') #Inconsistent Unicode \n",
    "\n",
    "#Replace all others with a space\n",
    "df['text'] = df['text'].str.replace(pattern_unusual, '', regex=True)\n",
    "\n",
    "ser_unusual = df['text'].str.contains(pattern_unusual)\n",
    "print(ser_unusual.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414299a",
   "metadata": {},
   "source": [
    "Next, we should identify the vocabulary size. This value is useful for determining the expected optimal value for the max tokens are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4f9209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 15617\n"
     ]
    }
   ],
   "source": [
    "words = df['text'].str.split()\n",
    "words = words.explode()\n",
    "\n",
    "vocab_size = words.nunique()\n",
    "\n",
    "print(f'Vocab Size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d9304",
   "metadata": {},
   "source": [
    "Next we create the model. This task is not particularly difficult in terms of complexity or computing power, so a light-weight framework using keras will do the job just fine. We make use of SKLearn's train-test split as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8627254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb8d8a",
   "metadata": {},
   "source": [
    "We need to convert the string data to numerical format. The must be done before the split so that we capture the entire vocabulary of the dataset. The max amount of characters in a sms message is 160, but tokens are done on a word-word basis, so we can truncate to a smaller amount. 50 should be more than enough.\n",
    "\n",
    "The max_tokens parameter is the number total number of words that are encoded into the vocabulary. These words are the top N most common in the dataset. This is a hyperparameter, where it must be tweaked for the optimal value, but we can use the vocabulary size to get a rough idea. There are 15632 unique words, so a value between 3000 and 10000 will probably capture the most semantic meaning of the words. The model runs quick, so it is easy to adjust until the value is optimal. 5000 tokens results in the best test set accuracy, which is approximately a 1:3 token-to-word ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d4a1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_all = df['text'].values\n",
    "y_all = df['label'].values\n",
    "\n",
    "vectorizer = layers.TextVectorization(max_tokens=5000,output_mode='int',output_sequence_length=50,encoding='utf-8')\n",
    "vectorizer.adapt(X_all) #Adapt the vectorizer before splitting so it learns the vocabulary of all values.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all,y_all,test_size=0.2,random_state=seed, stratify=y_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f472b1",
   "metadata": {},
   "source": [
    "The model architecture is a simple 1D convulutional model with 2 fully connected layers. The dropout here is high, as this task is very prone to overfitting.\n",
    "\n",
    "We include false positives in the metrics, as it is important that legitimate texts are not marked as spam at a high rate.\n",
    "\n",
    "We also use a callback to set an early stopping condition. This model does not need many epochs to train, but we want to ensure we use the one with the best validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d15bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8802 - false_positives: 24.0000 - loss: 0.3561 - val_accuracy: 0.9731 - val_false_positives: 6.0000 - val_loss: 0.1035\n",
      "Epoch 2/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9776 - false_positives: 27.0000 - loss: 0.0991 - val_accuracy: 0.9787 - val_false_positives: 13.0000 - val_loss: 0.0815\n",
      "Epoch 3/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9913 - false_positives: 9.0000 - loss: 0.0528 - val_accuracy: 0.9865 - val_false_positives: 5.0000 - val_loss: 0.0676\n",
      "Epoch 4/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9950 - false_positives: 6.0000 - loss: 0.0364 - val_accuracy: 0.9843 - val_false_positives: 6.0000 - val_loss: 0.0701\n",
      "Epoch 5/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9958 - false_positives: 4.0000 - loss: 0.0290 - val_accuracy: 0.9877 - val_false_positives: 2.0000 - val_loss: 0.0695\n",
      "Epoch 6/10\n",
      "\u001b[1m446/446\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9983 - false_positives: 1.0000 - loss: 0.0163 - val_accuracy: 0.9854 - val_false_positives: 1.0000 - val_loss: 0.0762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x227c86bb380>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorizer,\n",
    "    layers.Embedding(input_dim=len(vectorizer.get_vocabulary())+1,output_dim=64),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Conv1D(64,7,padding='valid',activation='relu',strides=3),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1,activation='sigmoid',kernel_regularizer=l2(0.01))  \n",
    "])\n",
    "\n",
    "stopping = EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy',keras.metrics.FalsePositives()])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=8, validation_split=0.2,callbacks=[stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadca51e",
   "metadata": {},
   "source": [
    "Next, we evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e48ed5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9785 - false_positives: 6.0000 - loss: 0.0935 \n",
      "[0.09349261969327927, 0.9784753322601318, 6.0]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc7700",
   "metadata": {},
   "source": [
    "The following loop shows how many false positives vs false negatives there are for different threshold levels. Increasing the threshold decreases the false positives but increases the false negatives. It is possible to get the same accuracy with no false positives at 95% threshold, and at 85% the accuracy is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f0f880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Threshold: 0.5 | FP: 6 | FN: 18\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.55 | FP: 6 | FN: 18\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.6000000000000001 | FP: 6 | FN: 18\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.6500000000000001 | FP: 5 | FN: 18\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.7000000000000002 | FP: 4 | FN: 18\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.7500000000000002 | FP: 4 | FN: 20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.8000000000000003 | FP: 4 | FN: 20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.8500000000000003 | FP: 2 | FN: 20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.9000000000000004 | FP: 1 | FN: 23\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Threshold: 0.9500000000000004 | FP: 0 | FN: 24\n"
     ]
    }
   ],
   "source": [
    "thresh = 0.5\n",
    "\n",
    "while thresh < 1.0:\n",
    "    predictions = model.predict(X_test)\n",
    "    y_pred = (predictions > thresh).astype(int).flatten()\n",
    "    fp_count = np.sum(np.logical_and(y_pred==1,y_test==0))\n",
    "    fn_count = np.sum(np.logical_and(y_pred==0,y_test==1))\n",
    "    print(f'Threshold: {thresh} | FP: {fp_count} | FN: {fn_count}')\n",
    "    thresh += 0.05\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe8915",
   "metadata": {},
   "source": [
    "Next we save the model. This is so we can use it for deployment, which involves making accesible via API. This is done in the main.py script, and launched inside a docker container for compatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db599da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('spamclass.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
